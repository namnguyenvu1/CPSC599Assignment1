{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wmUvsFUWWfbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Using the Sigmoid as a template, implement the following activataion functions:**\n",
        "\n",
        "  a. ReLU\n",
        "\n",
        "  b. Tanh\n",
        "\n",
        "  c. [EXPONENTIAL LINEAR UNITS (ELUS)](https://arxiv.org/pdf/1511.07289.pdf)  \n",
        "\n",
        "  d. [Mish](https://arxiv.org/pdf/1908.08681.pdf)\n"
      ],
      "metadata": {
        "id": "uJj66woixvh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.output = 1.0 / (1.0 + np.exp(-x))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dL_dy):\n",
        "        dy_dx = self.output * (1 - self.output)\n",
        "        return dL_dy * dy_dx"
      ],
      "metadata": {
        "id": "YaAoahDGsf1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "- `forward`: Takes in an input $x$ and computes the sigmoid activation. It also stores the output for use in the backward pass. We will learn about that later in the course and also in the next assignement.\n",
        "  \n",
        "- `backward`: Takes in the upstream gradient (usually represented as $\\frac{dL}{dy}$, where $L $ is the loss and $y$ is the output of the sigmoid) and multiplies it by the local gradient (the derivative of the sigmoid) to produce the gradient of the loss with respect to the input, $\\frac{dL}{dx}$. Note that $\\frac{dL}{dy}$ is typically denoted as `grad_output` in many PyTorch examples. It is also called upstream gradient.\n",
        "\n",
        "For learning purposes or if you're trying to implement certain custom functionalities not provided by PyTorch, then the object-oriented approach with `forward` and `backward` methods is a great way to go. In this assignment, we must rely on Numpy for implementation, avoiding the high-level frameworks like PyTorch or TensorFlow."
      ],
      "metadata": {
        "id": "nl7z2FnZstSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement ReLu class\n",
        "class ReLu:\n",
        "  def __init__(self):\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.output = np.maximum(0, x)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, dL_dy):\n",
        "    dy_dx = np.where(self.output > 0, 1, 0)\n",
        "    return dL_dy * dy_dx"
      ],
      "metadata": {
        "id": "7OaS70UTFWbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement Thanh class\n",
        "class Thanh:\n",
        "  def __init__(self):\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.output = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, dL_dy):\n",
        "    dy_dx = 1 - (self.output)**2\n",
        "    return dL_dy * dy_dx"
      ],
      "metadata": {
        "id": "iQ1C9efKFd8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement ELUS class\n",
        "class ELUS:\n",
        "  def __init__(self, alpha=1):  # The default value of ELU's alpha = 1(follow the torch documentation), it can be alter later if needed\n",
        "  # If alpha not initialize to 1, then it doesn't look like real implementation as you need to set it to 1 anytime you use it\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    self.output = np.where(self.input > 0, x, self.alpha * (np.exp(x) - 1))\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, dL_dy):\n",
        "    dy_dx = np.where(self.input > 0, 1, self.alpha * np.exp(self.input))\n",
        "    return dL_dy * dy_dx"
      ],
      "metadata": {
        "id": "M6zYrCMbFjeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement Mish class\n",
        "class Mish:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    self.output = x * np.tanh(np.log(1 + np.exp(x)))\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, dL_dy):\n",
        "    x = self.input\n",
        "    nominator = np.exp(x) * (4*(x+1) + 4*np.exp(2*x) + np.exp(3*x) + np.exp(x)*(4*x+6))\n",
        "    denominator = (2*np.exp(x) + np.exp(2*x) + 2)**2\n",
        "    dy_dx = nominator/denominator\n",
        "    return dL_dy * dy_dx"
      ],
      "metadata": {
        "id": "sU5NFycbFpYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Implement the CrossEntropyLoss Class.**\n",
        "\n",
        "In many deep learning tasks, especially classification, the CrossEntropyLoss function plays a pivotal role. For this assignment, your task is to implement the `CrossEntropyLoss` class with both `forward` and `backward` methods.\n",
        "\n",
        "Specifications:\n",
        "\n",
        "1. **Input**:\n",
        "    - `logits`: A 2D numpy array of shape `(batch_size, num_classes)`. Each row represents the logit scores (pre-softmax outputs) for each class of a particular sample.\n",
        "    - `target`: A 2D numpy array of shape `(batch_size, num_classes)`, representing the one-hot encoded true labels.\n",
        "\n",
        "2. **Output**:\n",
        "    - The `forward` method should return a scalar representing the average loss over the entire batch.\n",
        "    - The `backward` method should return the gradient of the loss with respect to the logits, which will be used for gradient-based optimization.\n",
        "\n",
        "3. **Numerical Stability**: Ensure that your implementation is numerically stable, avoiding potential pitfalls like overflow or underflow in the softmax and logarithm calculations.\n",
        "\n",
        "Note:\n",
        "- CrossEntropyLoss is typically computed using logit inputs, i.e., the outputs of the model before the softmax function is applied. This is for computational efficiency.\n"
      ],
      "metadata": {
        "id": "33jMkIWiF01p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Implement the CrossEntropyLoss here\n",
        "class CrossEntropyLoss:\n",
        "  def __init__(self):\n",
        "    self.logits = None\n",
        "    self.target = None\n",
        "\n",
        "  def forward(self, logits, target):\n",
        "    self.logits = logits\n",
        "    self.target = target\n",
        "\n",
        "    logitStable = logits - np.max(logits) # Substract the max value to make the logits stable (for numerical stability)\n",
        "    self.logits = logitStable # Save stable value to self.logits\n",
        "    logitAfterSoftmax = np.exp(logitStable) / np.sum(np.exp(logitStable), axis=1, keepdims=True)\n",
        "    # The formula for logit after applying Softmax is logits/(sigma sum of logits)\n",
        "    cross_entropy = -np.sum(self.target * np.log(logitAfterSoftmax)) # Without the negative sign, the loss will be negative\n",
        "    loss = cross_entropy/len(logitStable)\n",
        "    return loss\n",
        "\n",
        "  def backward(self):\n",
        "    logitAfterSoftmax = np.exp(self.logits) / np.sum(np.exp(self.logits), axis=1, keepdims=True)  #This logits is logit after softmax\n",
        "    gradient = (logitAfterSoftmax-self.target)/len(self.logits)\n",
        "    return gradient"
      ],
      "metadata": {
        "id": "vQNFIwVRWAu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the test code\n",
        "logits = np.ones(shape=(10,3))\n",
        "target = np.ones(shape=(10,3))\n",
        "myCrossEntropyLoss = CrossEntropyLoss()\n",
        "answers = myCrossEntropyLoss.forward(logits=logits, target=target)\n",
        "print(answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiIcd3GVNwCM",
        "outputId": "cb1bee83-cdd9-43ae-f1e2-52091f9e3dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.29583686600433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3. Implement a Simple ANN Class**\n",
        "\n",
        "Write a Python class named `ANN` that simulates a basic feed-forward artificial neural network.\n",
        "\n",
        "Specifications:\n",
        "\n",
        "1. **Initialization (`__init__` method)**:\n",
        "    - The network should be initialized with 784 inputs and 10 outputs.\n",
        "    - It must have two hidden layers, each containing 256 neurons.\n",
        "    - Use the ReLU activation function for each hidden layer.\n",
        "    - Initialize weights using a gaussian distribution with zero mean a variance of `2 / (number of input nodes + number of output nodes)`.\n",
        "    - Include bias terms for each neuron and initialize them to zero.\n",
        "\n",
        "2. **Forward Propagation (`forward` method)**:\n",
        "    - The method should take a batch of `k` inputs (a 2D numpy array of shape `(k, 784)`).\n",
        "    - It should process the batch through the network and produce a batch of `k` logit outputs, each being a vector of size 10 (thus, the output shape should be `(k, 10)`).\n",
        "\n",
        "Note: For this assignment, use `numpy` for all mathematical operations.\n",
        "\n",
        "---\n",
        "\n",
        "This version provides a clearer set of requirements and expectations for the students."
      ],
      "metadata": {
        "id": "XZHl_GeFHii_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the ANN class here\n",
        "class ANN:\n",
        "  def __init__(self):\n",
        "    self.inputs=784\n",
        "    self.hidden=256\n",
        "    self.outputs=10\n",
        "    self.weight1 = np.random.normal(loc=0, scale=np.sqrt(2/(self.inputs+self.hidden)), size=(self.inputs, self.hidden))\n",
        "    self.weight2 = np.random.normal(loc=0, scale=np.sqrt(2/(self.hidden+self.hidden)), size=(self.hidden, self.hidden))\n",
        "    self.weight3 = np.random.normal(loc=0, scale=np.sqrt(2/(self.hidden+self.outputs)), size=(self.hidden, self.outputs))\n",
        "    # In the weight calculation, loc is the mean, scale is the standard deviation (so I sqrt the variance), size is the size of the weight\n",
        "    # Because zero mean so loc=0, variance is given so scale (standard deviation) is the sprt of that\n",
        "\n",
        "    self.bias1 = np.zeros(shape=(1,self.hidden))\n",
        "    self.bias2 = np.zeros(shape=(1,self.hidden))\n",
        "    self.bias3 = np.zeros(shape=(1,self.outputs))\n",
        "\n",
        "  def forward(self, X):\n",
        "    my_relu = ReLu()  # I call my relu function above so all cells have to be run!\n",
        "    self.hiddenlayer1_nonactivation = np.dot(X, self.weight1) + self.bias1\n",
        "    self.hiddenlayer1 = my_relu.forward(self.hiddenlayer1_nonactivation)\n",
        "    self.hiddenlayer2_nonactivation = np.dot(self.hiddenlayer1, self.weight2) + self.bias2\n",
        "    self.hiddenlayer2 = my_relu.forward(self.hiddenlayer2_nonactivation)\n",
        "    self.hiddenlayer3 = np.dot(self.hiddenlayer2, self.weight3) + self.bias3\n",
        "    return self.hiddenlayer3"
      ],
      "metadata": {
        "id": "v6Wujx8yKkPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the test code\n",
        "model1 = ANN()\n",
        "X = np.ones(shape=(6, 784))\n",
        "result = model1.forward(X)\n",
        "print(result.shape)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Ebpjiz_yMt",
        "outputId": "59dd9a90-90c4-49c5-a036-adc447611624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 10)\n",
            "[[-0.83941167 -0.26125574  1.90310268  0.36106172  0.96642744  1.15667227\n",
            "   0.20331913  0.15909078  0.62160771  1.03611572]\n",
            " [-0.83941167 -0.26125574  1.90310268  0.36106172  0.96642744  1.15667227\n",
            "   0.20331913  0.15909078  0.62160771  1.03611572]\n",
            " [-0.83941167 -0.26125574  1.90310268  0.36106172  0.96642744  1.15667227\n",
            "   0.20331913  0.15909078  0.62160771  1.03611572]\n",
            " [-0.83941167 -0.26125574  1.90310268  0.36106172  0.96642744  1.15667227\n",
            "   0.20331913  0.15909078  0.62160771  1.03611572]\n",
            " [-0.83941167 -0.26125574  1.90310268  0.36106172  0.96642744  1.15667227\n",
            "   0.20331913  0.15909078  0.62160771  1.03611572]\n",
            " [-0.83941167 -0.26125574  1.90310268  0.36106172  0.96642744  1.15667227\n",
            "   0.20331913  0.15909078  0.62160771  1.03611572]]\n"
          ]
        }
      ]
    }
  ]
}